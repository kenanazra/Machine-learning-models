{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a2f1a1-aa32-456e-a7f1-c8d3d8a33263",
   "metadata": {},
   "source": [
    "# Machine learning models\n",
    "This project offers a diverse selection of classification machine learning models. Users can choose models manually or opt for automatic selection based on their specific needs and analysis requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ddffb8-5a1b-4abc-8f72-b6773f8044db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error\n",
    "from scipy import stats as st\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823c4fc5-e6b6-4546-b652-9c4c779e57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c185c51e-6482-466d-a5ca-25e7c74014d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237b153-4e6d-4a87-b437-4cf458c922ad",
   "metadata": {},
   "source": [
    "## Model-Aufbau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c5e81-8592-48d4-80c0-466ed928c58a",
   "metadata": {},
   "source": [
    "### automatisches Auswahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae42dd8-6fef-4ac2-998d-e21427528755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatisches Auswahl\n",
    "def model_TPOTClassifier(X_train,y_train):\n",
    "    tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "    tpot.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18cf30-1c6b-45d1-ad15-d490c473f05c",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de95e69d-862d-48ce-b980-89bde2a84e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "def model_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Ergebnisse auswerten\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return model  # <-- Hier geben wir das Modell zurück!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b6ddf-3384-4796-b198-902c12a8f904",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669baa97-3124-4afc-b136-1f80fdda777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "def model_svm(X_train, y_train, X_test, y_test):\n",
    "    param_grid = {'C': [0.1, 1, 10, 100],\n",
    "                  'gamma': ['scale', 'auto'],\n",
    "                  'kernel': ['linear', 'rbf']}\n",
    "    \n",
    "    grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Ergebnisse auswerten\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2bc35-d59a-4957-a2bb-a112bf6cb9b6",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f353fd11-cbb2-4616-8313-13742da0638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "def model_knn(X_train, y_train, X_test, y_test):\n",
    "    param_grid = {'n_neighbors': np.arange(1, 31),\n",
    "                  'weights': ['uniform', 'distance']}\n",
    "    \n",
    "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_knn = grid.best_estimator_\n",
    "    y_pred   = best_knn.predict(X_test)\n",
    "    \n",
    "    # Ergebnisse auswerten\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return best_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f91aa-4ef0-4028-8d3a-f714bc300c9c",
   "metadata": {},
   "source": [
    "### Naive Bayes ( GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1643f215-dc69-4ba3-9a55-6f8b0964ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes ( GaussianNB)\n",
    "def model_gaussian_nb(X_train, y_train, X_test, y_test):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Ergebnisse auswerten\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec6ca6-21b9-496b-954d-72bd03c9da2f",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d75ef971-e380-4c92-96a5-8ee1e56c39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "def model_decision_tree(X_train, y_train, X_test, y_test):\n",
    "    param_grid = {'max_depth': [3, 4, 5, 6, 7, None],\n",
    "                  'min_samples_split': [2, 5, 10],\n",
    "                  'min_samples_leaf': [1, 2, 4],\n",
    "                  'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    #print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    #print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "    y_pred   = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    # Ergebnisse auswerten\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7e4eb-3a97-4695-8249-6850439f4eec",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d400d0-7961-4620-aeee-15be3af9a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "def model_Random_forest(X_train, y_train, X_test, y_test):\n",
    "    param_grid = {\n",
    "    'n_estimators': [10,100, 200, 300],\n",
    "    #'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    #'min_samples_leaf': [1, 2, 4],\n",
    "    #'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Erstelle das GridSearchCV-Objekt\n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Führe Grid Search aus\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Beste Parameter\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred   = best_model.predict(X_test)\n",
    "\n",
    "    # Ergebnisse auswerten\n",
    "    mse      = mean_squared_error(y_test,y_pred)\n",
    "    mae      = mean_absolute_error(y_test,y_pred)\n",
    "    score_r2 = r2_score(y_test,y_pred)\n",
    "    rmse     = root_mean_squared_error(y_test,y_pred)\n",
    "\n",
    "    #print(mse)\n",
    "    #print(mae)\n",
    "    print(f'score_r2 = {score_r2}')\n",
    "    #print(rmse)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9ce5c-7468-4bbe-aeb7-ad15ae4b6b66",
   "metadata": {},
   "source": [
    "### validation_techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857aaddc-20c3-47c9-8aaa-09b19ac73990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation_techniques(method, model, X, y):\n",
    "    if model is None:\n",
    "        print(\"Fehler: Kein trainiertes Modell übergeben!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Validierungsmethode: {method}\")\n",
    "    \n",
    "    if method == '1':  # K-Fold Cross Validation\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "        results = cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "    elif method == '2':  # Stratified K-Fold Cross Validation\n",
    "        skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "        results = cross_val_score(model, X, y, cv=skfold)\n",
    "\n",
    "    elif method == '3':  # Leave-One-Out Cross Validation\n",
    "        loo = LeaveOneOut()\n",
    "        results = cross_val_score(model, X, y, cv=loo)\n",
    "\n",
    "    elif method == '4':  # Leave-P-Out Cross Validation\n",
    "        lpo = LeavePOut(p=5)\n",
    "        results = cross_val_score(model, X, y, cv=lpo)\n",
    "    elif method == '5':  # Repeated Random Test-Train Splits\n",
    "        ssplit = ShuffleSplit(n_splits=20, test_size=0.20, random_state=1)\n",
    "        results = cross_val_score(model, X, y, cv=ssplit)\n",
    "\n",
    "    else:\n",
    "        print('Bitte eine gültige Methode wählen.')\n",
    "        return\n",
    "    \n",
    "    print(\"Ergebnisse der Cross-Validation:\")\n",
    "    print(results)\n",
    "    print(\"Durchschnittliche Genauigkeit:\", np.mean(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba375a-7507-4cbd-ae84-5f91625ed39c",
   "metadata": {},
   "source": [
    "### Max Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c2f8353-7959-44d9-b752-6ee6a4a7c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funktion Max Voting\n",
    "def model_Max_Voting(model_switch, X_train, y_train, X_test, y_test):\n",
    "    print('Max Voting wurde ausgewählt.')\n",
    "    \n",
    "    # Benutzer kann die Modelle auswählen\n",
    "    print(\"Verfügbare Modelle:\")\n",
    "    for key in model_switch:\n",
    "        if key != '6' and key != '7':  # '7' ist Max Voting, es wird nicht zur Auswahl angezeigt\n",
    "            print(f\"{key}: {model_switch[key].__name__}\")\n",
    "    \n",
    "    selected_models = input(\"Bitte wählen Sie die Modelle für Max Voting (z.B. 1,2,3): \").split(',')\n",
    "    selected_models = [m.strip() for m in selected_models]  # Entfernen von Leerzeichen\n",
    "\n",
    "    # Überprüfen der Gültigkeit der Eingaben\n",
    "    valid_models = []\n",
    "    for model_key in selected_models:\n",
    "        if model_key in model_switch and model_key != '7':\n",
    "            valid_models.append(model_key)\n",
    "        else:\n",
    "            print(f\"Modell {model_key} ist ungültig und wird übersprungen.\")\n",
    "\n",
    "    if not valid_models:\n",
    "        print(\"Keine gültigen Modelle ausgewählt. Abbruch.\")\n",
    "        return None\n",
    "\n",
    "    # Training der ausgewählten Modelle und Sammeln der Vorhersagen\n",
    "    predictions = []\n",
    "    for model_key in valid_models:\n",
    "        model_function = model_switch[model_key]  # Funktion aus model_switch abrufen\n",
    "        print(f\"Training Modell {model_key} ({model_function.__name__})...\")\n",
    "        trained_model = model_function(X_train, y_train, X_test, y_test)  # Modell trainieren\n",
    "        y_pred = trained_model.predict(X_test)\n",
    "        predictions.append(y_pred)\n",
    "    \n",
    "    # Umwandlung der Vorhersagen in ein NumPy-Array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Berechnung der finalen Vorhersage mit mode()\n",
    "    final_pred, _ = st.mode(predictions, axis=0)\n",
    "\n",
    "    # Sicherstellen, dass final_pred die richtige Form hat\n",
    "    final_pred = final_pred.ravel()\n",
    "\n",
    "    print(\"Final Prediction:\")\n",
    "    print(final_pred)\n",
    "\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, final_pred))\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, final_pred))\n",
    "\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2fd14-cb95-4d14-8a19-2e73ad326930",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1fa417-283d-420d-97f0-43c3758f029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion für Stacking\n",
    "def model_Stacking(model_switch, X_train, y_train, X_test, y_test):\n",
    "    print('Stacking wurde ausgewählt.')\n",
    "    \n",
    "    # Benutzer kann die Modelle auswählen\n",
    "    print(\"Verfügbare Modelle:\")\n",
    "    for key in model_switch:\n",
    "        if key != '6' and key != '7' and key != '8' and key != '9':  # '7' ist Max Voting, es wird nicht zur Auswahl angezeigt\n",
    "            print(f'{key}: {model_switch[key].__name__}')\n",
    "    \n",
    "    selected_models = input('Bitte wählen Sie die Modelle für Stacking (z.B. 1,2,3): ').split(',')\n",
    "    selected_models = [m.strip() for m in selected_models]  # Entfernen von Leerzeichen\n",
    "    print(selected_models)\n",
    "    # Überprüfen der Gültigkeit der Eingaben\n",
    "    valid_models = []\n",
    "    for model_key in selected_models:\n",
    "        if model_key in model_switch and model_key != '7':\n",
    "            valid_models.append(model_key)\n",
    "        else:\n",
    "            print(f'Modell {model_key} ist ungültig und wird übersprungen.')\n",
    "\n",
    "    if not valid_models:\n",
    "        print('Keine gültigen Modelle ausgewählt. Abbruch.')\n",
    "        return None\n",
    "\n",
    "    # Training der ausgewählten Modelle und Sammeln der Modelle für Stacking\n",
    "    base_models = []\n",
    "    for model_key in valid_models:\n",
    "        model_function = model_switch[model_key]  # Funktion aus model_switch abrufen\n",
    "        print(f\"Training Modell {model_key} ({model_function.__name__})...\")\n",
    "        trained_model = model_function(X_train, y_train, X_test, y_test)  # Modell trainieren\n",
    "        base_models.append((f\"model_{model_key}\", trained_model))\n",
    "    print(base_models)\n",
    "    # Definition des Meta-Modells\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Erstellen des Stacking Classifiers\n",
    "    stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "    # Training des Stacking Classifiers\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Vorhersage mit dem Stacking Classifier\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Evaluierung des Modells\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Stacking Classifier Accuracy: {accuracy}')\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade589a5-0631-4e8c-9b8c-80f0c2d40faf",
   "metadata": {},
   "source": [
    "###  Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ff5ecec-eb41-4020-a8ed-3b60d1a21061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion Bagging\n",
    "def model_Bagging(model_switch, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    print('Bagging wurde ausgewählt.')\n",
    "    \n",
    "    # Benutzer kann die Modelle auswählen\n",
    "    print(\"Verfügbare Modelle:\")\n",
    "    for key in model_switch:\n",
    "        if key != '6' and key != '7' and key != '8' and key != '9':  # '7' ist Max Voting, es wird nicht zur Auswahl angezeigt\n",
    "            print(f'{key}: {model_switch[key].__name__}')\n",
    "    \n",
    "    selected_models = input('Bitte wählen Sie die Modelle für Bagging : ').split(',')\n",
    "    selected_models = [m.strip() for m in selected_models]  # Entfernen von Leerzeichen\n",
    "    print(selected_models)\n",
    "    \n",
    "    # Überprüfen der Gültigkeit der Eingaben\n",
    "    valid_models = []\n",
    "    for model_key in selected_models:\n",
    "        if model_key in model_switch and model_key != '7':\n",
    "            valid_models.append(model_key)\n",
    "        else:\n",
    "            print(f'Modell {model_key} ist ungültig und wird übersprungen.')\n",
    "\n",
    "    if not valid_models:\n",
    "        print('Keine gültigen Modelle ausgewählt. Abbruch.')\n",
    "\n",
    "    # Training der ausgewählten Modelle und Sammeln der Modelle für Bagging\n",
    "    model_function = model_switch[model_key]  \n",
    "    print(f\"Training Modell {model_key} ({model_function.__name__})...\")\n",
    "    trained_model = model_function(X_train, y_train, X_test, y_test) \n",
    "    #print(trained_model)\n",
    "    # Hyperparameter-Raster für BaggingClassifier\n",
    "    bc_params = {\n",
    "        'n_estimators': [10, 20, 30],\n",
    "        'max_samples': [0.5, 0.7, 1.0],\n",
    "        'max_features': [0.5, 0.7, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Grid Search für BaggingClassifier\n",
    "    bagging_clf = BaggingClassifier(trained_model)\n",
    "    bc_gs = GridSearchCV(bagging_clf, bc_params, cv=5, verbose=1)\n",
    "\n",
    "    bc_gs.fit(X_train, y_train)\n",
    "    y_pred = bc_gs.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'BaggingClassifier Accuracy: {accuracy}')\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc0f8e-6ed2-4607-bc0b-64afd89c5122",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5f9557e-a7ca-4b52-a7cd-e2bfd33dd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion Boosting\n",
    "def model_Boosting(model_switch, X_train, y_train, X_test, y_test):\n",
    "    print('Boosting wurde ausgewählt.')\n",
    "    # Hyperparameter-Raster für GradientBoostingClassifier\n",
    "    gbc_params = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_features': [None, 'sqrt', 'log2'],\n",
    "         'max_depth': [3, 5, 7],\n",
    "        #'min_samples_split': [2, 5, 10],\n",
    "        #'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Grid Search für GradientBoostingClassifier\n",
    "    gbc = GradientBoostingClassifier(random_state=100)\n",
    "    gbc_gs = GridSearchCV(gbc, gbc_params, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "    gbc_gs.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = gbc_gs.predict(X_test)\n",
    "\n",
    "    # Modell evaluieren\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'GradientBoostingClassifier Accuracy: {accuracy}')\n",
    "    print(\"Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Konfusionsmatrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return gbc_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5dad772-73b0-4b1c-a2e2-46493f92517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "um alle Merkmale eines Datensatzes auf einem ähnlichen Maßstab liegen, kann Feature-Scaling verwendet werden. Dies ist besonders wichtig für Algorithmen wie k-Nearest Neighbors oder Support Vector Machines, die auf Abständen basieren. Durch das Skalieren der Features wird verhindert,  dass Merkmale mit größeren Wertebereichen die Ergebnisse dominieren. Zu den gängigen Methoden gehören die Min-Max-Skalierung, die Werte in einen Bereich von 0 bis 1 transformiert, und die Z-Score-Normalisierung, die die Werte so anpasst, dass sie einen Mittelwert von 0 und eine Standardabweichung von 1 haben. Insgesamt verbessert Feature-Scaling die Leistung und Genauigkeit von Machine-Learning-Modellen.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Möchten Sie ein Feature-Scaling durchführen (Ja/Nein) nein\n",
      "Soll das ML-Modell automatisch oder manuell ausgewählt werden? manuell\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wählen Sie ein Modell (1-10):\n",
      "1: Logistic Regression\n",
      "2: Support Vector Machines\n",
      "3: K-Nearest Neighbors\n",
      "4: Naive Bayes\n",
      "5: Decision Trees\n",
      "6: Random Forest\n",
      "7: Max Voting\n",
      "8: Model Stacking\n",
      "9: Model Bagging\n",
      "10: Model Boosting\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wählen Sie bitte eine Nummer von der Liste aus:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sie haben Modell 2 gewählt.\n",
      "Die Bearbeitung des Modells kann etwas länger dauern\n",
      "Klassifikationsbericht:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92        42\n",
      "           1       0.92      1.00      0.96        72\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.96      0.93      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "Konfusionsmatrix:\n",
      "[[36  6]\n",
      " [ 0 72]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Als Validierungstechnik wurde die Methode Train-Test-Split durchgeführt. Möchten Sie eine andere Validierungstechnik verwenden? (Ja/Nein)  nein\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Aufteilen in Trainings- und Testdaten mit Train-Test-Split durchführen\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print ('um alle Merkmale eines Datensatzes auf einem ähnlichen Maßstab liegen, kann Feature-Scaling verwendet werden.'\n",
    "       ' Dies ist besonders wichtig für Algorithmen wie k-Nearest Neighbors oder Support Vector Machines,'\n",
    "       ' die auf Abständen basieren. Durch das Skalieren der Features wird verhindert, '\n",
    "       ' dass Merkmale mit größeren Wertebereichen die Ergebnisse dominieren.'\n",
    "       ' Zu den gängigen Methoden gehören die Min-Max-Skalierung,'\n",
    "       ' die Werte in einen Bereich von 0 bis 1 transformiert,'\n",
    "       ' und die Z-Score-Normalisierung, die die Werte so anpasst,'\n",
    "       ' dass sie einen Mittelwert von 0 und eine Standardabweichung von 1 haben.'\n",
    "       ' Insgesamt verbessert Feature-Scaling die Leistung und Genauigkeit von Machine-Learning-Modellen.')\n",
    "        \n",
    "# Feature-Scaling\n",
    "Feature_Scaling = input('Möchten Sie ein Feature-Scaling durchführen (Ja/Nein)')\n",
    "if Feature_Scaling.lower() == 'ja':\n",
    "    Feature_Scaling_Methode = input('welche Methode möchten Sie durchführen (Standardization/Normalization)?')\n",
    "    while Feature_Scaling_Methode.lower() != 'standardization' and Feature_Scaling_Methode.lower() != 'normalization':\n",
    "        print('Wählen Sie eine Methode von der Liste aus')\n",
    "        Feature_Scaling_Methode = input('welche Methode möchten Sie durchführen (Standardization/Normalization)?')\n",
    "            \n",
    "    if Feature_Scaling_Methode.lower() == 'standardization':\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "                \n",
    "    elif Feature_Scaling_Methode.lower() == 'normalization':\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        X_train = min_max_scaler.fit_transform(X_train)\n",
    "        X_test = min_max_scaler.transform(X_test)\n",
    "        \n",
    "\n",
    "model_wahl = input('Soll das ML-Modell automatisch oder manuell ausgewählt werden?')\n",
    "\n",
    "if model_wahl == 'automatisch':\n",
    "    Evaluation = model_TPOTClassifier(X_train, y_train)\n",
    "else:\n",
    "\n",
    "    model_options = [\n",
    "    '1: Logistic Regression', \n",
    "    '2: Support Vector Machines', \n",
    "    '3: K-Nearest Neighbors', \n",
    "    '4: Naive Bayes', \n",
    "    '5: Decision Trees', \n",
    "    '6: Random Forest', \n",
    "    '7: Max Voting', \n",
    "    '8: Model Stacking', \n",
    "    '9: Model Bagging', \n",
    "    '10: Model Boosting'\n",
    "    ]\n",
    "\n",
    "    print('Wählen Sie ein Modell (1-10):')\n",
    "    for option in model_options:\n",
    "        print(option)\n",
    "    model_wahl = input()\n",
    "    \n",
    "    valid_choices = [str(i) for i in range(1, 11)]\n",
    "\n",
    "    while model_wahl not in valid_choices:\n",
    "        print('Wählen Sie bitte eine Nummer von der Liste aus:')\n",
    "        model_wahl = input()\n",
    "\n",
    "    print(f'Sie haben Modell {model_wahl} gewählt.')\n",
    "    \n",
    "    model_switch = {\n",
    "        '1': model_logistic_regression,\n",
    "        '2': model_svm, \n",
    "        '3': model_knn,\n",
    "        '4': model_gaussian_nb,\n",
    "        '5': model_decision_tree,\n",
    "        '6': model_Random_forest,\n",
    "        '7': model_Max_Voting,\n",
    "        '8': model_Stacking,\n",
    "        '9': model_Bagging,\n",
    "        '10': model_Boosting\n",
    "    }\n",
    "\n",
    "    if model_wahl in model_switch:\n",
    "        if model_wahl == '7':\n",
    "            Evaluation = model_Max_Voting(model_switch, X_train, y_train, X_test, y_test)\n",
    "                # Bild anzeigen\n",
    "            img = plt.imread('max_voting_image.png')  # Ersetzen Sie 'max_voting_image.png' durch den Namen Ihrer Bilddatei\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        elif model_wahl == '8':\n",
    "            Evaluation = model_Stacking(model_switch, X_train, y_train, X_test, y_test)\n",
    "            # Bild anzeigen\n",
    "            img = plt.imread('stacking_image.png')  # Ersetzen Sie 'max_voting_image.png' durch den Namen Ihrer Bilddatei\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        elif model_wahl == '9':\n",
    "            Evaluation = model_Bagging(model_switch, X_train, y_train, X_test, y_test)\n",
    "            img = plt.imread('Bagging_image.png')\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        elif model_wahl == '10':\n",
    "            Evaluation = model_Boosting(model_switch, X_train, y_train, X_test, y_test)\n",
    "            img = plt.imread('Boosting_image.png')\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            model_function = model_switch[model_wahl]\n",
    "            print('Die Bearbeitung des Modells kann etwas länger dauern')\n",
    "            trained_model = model_function(X_train, y_train, X_test, y_test)\n",
    "\n",
    "            val_techniques = input('Als Validierungstechnik wurde die Methode Train-Test-Split durchgeführt. Möchten Sie eine andere Validierungstechnik verwenden? (Ja/Nein) ')\n",
    "\n",
    "            if val_techniques.lower() == 'ja':\n",
    "                print('Wählen Sie eine Methode (1-5)')\n",
    "                print('1: K-fold')\n",
    "                print('2: Stratified K-fold')\n",
    "                print('3: LOOCV')\n",
    "                print('4: LPOCV')\n",
    "                print('5: RRTTS')\n",
    "    \n",
    "                val_techniques_wahl = input()\n",
    "    \n",
    "                valid_choices = ['1', '2', '3', '4', '5']\n",
    "    \n",
    "                while val_techniques_wahl not in valid_choices:\n",
    "                    print('Wählen Sie bitte eine Methode von der Liste aus.')\n",
    "                    val_techniques_wahl = input()\n",
    "\n",
    "                print('das kann etwas länge dauern')\n",
    "                # Modell übergeben\n",
    "                validation_techniques(val_techniques_wahl, trained_model, X, y)\n",
    "\n",
    "    else:\n",
    "        print('bitte ein Model von der Liste aussuchen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
